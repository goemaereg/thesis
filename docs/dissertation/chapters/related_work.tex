\chapter{Related work} \label{ch:related_work}
\section{Object recognition}
\acrfull{cnn}s are used with great performance in computer vision tasks. Within the landscape of object recognition tasks, given an input image, classification models return image-level class predictions \cite{lecun1998gradient}, semantic segmentation models \cite{shelhamer2017fully} produce pixel-wise class predictions, and object detection models \cite{girshick2014rich, girshick2015fast, ren2015faster} output a set of bounding-boxes with class predictions, one bounding box and class per recognized object. Object localization \cite{russakovsky2015imagenet} assumes that the image contains an object of a single class and produces a binary mask or a bounding box around the object of the class of interest.

\section{Weakly supervised object localization}
In supervised learning, a model learns a task by minimizing the error between its predictions and ground truth labels. Human labeling of bounding boxes and segmentation masks costs significantly more than labeling image-level categories \cite{papandreou2015weakly}. In contrast with object detection, \acrshort{wsol} only requires image-level class labels as ground truth. As a consequence,  \acrshort{wsol} research \cite{zhou2016cvpr, zhang2018adversarial, zhang2018self, choe2019attention, singh2017hide} has gained significant momentum.

\section{CAM-based WSOL methods}
\subsection{Class Activation Mapping}
An important \acrshort{wsol} method and baseline for a family of \acrshort{cam} methods was introduced by Zhou \textit{et al.} \cite{zhou2016cvpr}. \acrfull{cam} uses feature map activations in a \acrlong{cnn} to localize the discriminative image regions despite being trained for a classification task. A disadvantage is that \acrshort{cam} requires modifications in network architectures to accommodate a \acrlong{gap} layer. This has as consequence that classification accuracy drops compared with the original architecture. Another disadvantage is that \acrshort{cam} can only partially localize objects due to its focus on discriminative parts of an image. 

\subsection{Gradient-based CAM}
New \acrshort{wsol} research focused on overcoming the limitations of \acrshort{cam}. \acrfull{gradcam}, proposed by Selvaraju \textit{et al.} \cite{selvaraju2017grad}, uses gradients of a target score with respect to feature maps of a convolutional layer to produce a localization map highlighting the important regions in an image concerning a certain object target. \acrshort{gradcam} is a generalization of \acrshort{cam}: It is applicable to a wide variety of \acrshort{cnn} architectures as it doesn't require a specific \acrshort{gap} layer. Off-the-shelf models can be evaluated for the \acrshort{wsol} task as no retraining is required. \acrshort{gradcam} is also used for making \acrshort{cnn}-based models more transparent by producing visual explanations. Further, \acrshort{gradcam} combines localizations with existing high-resolution visualizations (Springenberg, J.T. \textit{et al.} \cite{springenberg2014striving}) to obtain high-resolution class-discriminative Guided Grad-CAM visualizations.

Chattopadhay \textit{et al.} \cite{chattopadhay2018grad, chattopadhyay2017grad} introduced Grad-CAM++, a generalization of \acrshort{gradcam}. It addresses Grad-CAM's shortcomings that multiple occurrences of objects of the same class in an image are poorly localized. As in \acrshort{gradcam}, the localization map of a certain class is a weighted combination of activation maps in the highest convolutional layer. In Grad-CAM, weights are an average of the gradients of an activation map. In Grad-CAM++, weights are a weighted average of the gradients of each pixel in the activation map. This way, parts of the image important for predicting a specific class will be highlighted more in the activation map. Grad-CAM++ provides better visual explanations for a given \acrshort{cnn} architecture when compared to Grad-CAM.

\subsection{CAM regularization}
One of the most common problems of \acrshort{cam} methods is caused either by localization maps which focus exclusively on the most discriminative region of the objects of interest, or by activations occurring in background regions. MinMaxCAM, proposed by Wang \textit{et al.} \cite{wang2021minmaxcam}, addresses these two problems by proposing two representation regularization mechanisms: Full Region Regularization which tries to maximize the coverage of the localization map inside the object region, and Common Region Regularization which minimizes the activations occurring in the background region. MinMaxCAM has the same architectural constraints as \acrshort{cam} in that it requires a modified network architecture with a \acrshort{gap} layer. An additional constraint is that it requires re-training due to its region regularization.

\subsection{Score-based CAM}
Wang \textit{et al.} \cite{wang2020score} propose a novel post-hoc visual explanation method based on class activation mapping, called Score-CAM. Unlike some previous \acrshort{cam} methods, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score for the target class. The final localization map, also known as score map, is obtained by a weighted combination of activation maps. Score-CAM achieves better visual performance and fairness for interpreting the decision-making process and outperforms previous methods on localization tasks. It also shows promise to recognize multiple object instances of the same class within an image. A disadvantage of the method is its high computational cost because it requires many additional forward passes to compute the weight of each activation map.

In this work, we will focus on CAM-based \acrshort{wsol} methods, because they have a remarkable localization ability despite being trained on image-level labels only.

\section{Non-CAM WSOL methods}
There has been a number of works other than \acrshort{cam} methods exploring \acrfull{wsol} using \acrshort{cnn}s. Bergamo \textit{et al.} \cite{bazzani2016self} proposes a self-taught object localization technique that masks out image regions to identify the region that incurs the maximal drop in recognition score, and hence has the highest likelihood to contain the object of interest. Cinbis \textit{et al.} \cite{cinbis2016weakly} and Pinheiro \textit{et al.} \cite{pinheiro2015image} combine multiple-instance learning with \acrshort{cnn}s to localize objects. While these approaches yield some promising results, they are not trained end-to-end.

We don't consider non-CAM WSOL methods in this work because they are not trained end-to-end and require multiple forward passes of a network to localize objects.

\section{WSOL evaluation}
In this section we discuss how each CAM-based method is evaluated by the authors of the method, to illustrate why their evaluation methods are not sufficient for evaluation localization of multiple object instances. We included evaluation of CAM-based model explanation methods because of their promising results to explain multiple object instances in images.

\subsection{Localization error}
Localization performance in \textbf{\acrshort{cam}} \cite{zhou2016cvpr} and \textbf{\acrshort{gradcam}} \cite{selvaraju2017grad} is computed by comparing the \acrfull{iou} between the ground truth bounding box and the bounding box that covers the largest connected component in the activation map with values of at least 20\% of the maximum value. An object is considered localized under the 0.5 \acrshort{iou} criterion, i.e., ground truth and predicted bounding box overlap at least 50\%. This computation is done for each of the top-5 predicted classes for the top-5 localization evaluation metric \cite{russakovsky2015imagenet}. The top-1 localization error is then the localization error for the top-1 predicted class.

\subsection{Faithfulness based metrics}
Chattopadhay \textit{et al.} \cite{chattopadhay2018grad} evaluate the faithfulness of generated explanations. An explanation map is generated by the Hadamard product \cite{million2007hadamard} of the upsampled class-discriminative saliency map with the original image. The explanation map is used as input to the model to evaluate three different metrics.

\subsubsection{Average drop \%}
This metric compares the average percentage drop in the model's prediction score for a particular class in an image when using the explanation map (i.e. after occlusion of the image with the saliency map) as input to the model. The better the explanation map keeps the most important image regions for a class, the less the drop of the prediction score is to be expected.

\subsubsection{\% increase in confidence}
This metric measures the number of times in a dataset the model's prediction score increased when occluding unimportant regions. This happens when all patterns a deep \acrshort{cnn} looks for are highlighted by the explanation map.

\subsubsection{Win\%}
This metric measures, in a given dataset, the number of times the drop in the model's confidence for an explanation map generated by a method 1 is less than for the explanation map generated by a method 2. A lower drop indicates that the explanation of method 1 is more model-appropriate than the explanation of method 2.

\subsection{Localization evaluation from energy-based perspective}
Score-CAM evaluates faithfulness of explanations using the same metrics as \acrshort{gradcam}++. For localization evaluation, Score-CAM uses a metric that treats localization from an energy based perspective: The metric computes the proportion of the sum of pixels in the saliency map that are inside a ground truth bounding box versus the sum of all pixels of the saliency map. For convenience, Score-CAM only considers these images with only one bounding box for a target class.

\subsection{Threshold-independent evaluation metrics}
Choe \textit{et al.} \cite{choe2020evaluating, choe2022evaluation} argue that using a fixed predefined threshold (Zhou \textit{et al.} \cite{zhou2016cvpr}) to produce a \acrshort{cam} localization region can be disadvantageous as the ideal threshold may depend on the data and architecture used. They discuss that prior evaluation metrics have failed to clearly measure localization performance. The localization accuracy metric \cite{russakovsky2015imagenet} and localization error metric \cite{zhou2016cvpr, selvaraju2017grad} combine classification and localization performances by counting the number of images where both tasks are performed correctly. They advocate to measure localization performance alone as the goal of \acrshort{wsol} is to localize objects and not to classify correctly. Hence, only score maps corresponding to ground truth classes are considered. Corresponding metrics are referred to as \textit{GT-known} metrics \cite{choe2019attention, singh2017hide, zhang2018adversarial, zhang2018self}. Choe \textit{et al.} propose evaluation metrics that are threshold-independent.

\subsubsection{Pixel-wise average precision}
When ground truth masks are available for evaluation, the pixel-wise precision and recall \cite{achanta2009frequency} are measured. This allows users to choose the preferred operating threshold that provides the best precision-recall trade-off. For threshold independence, the \acrfull{pxap} is defined as the area under the pixel-wise precision-recall curve.

\subsubsection{Maximum box accuracy}
Given availability of a ground truth bounding box, the box accuracy at a specific score map threshold and for a specific \acrshort{iou} threshold is then the number of images in a dataset where the overlap  between the bounding box of the largest connected component of the thresholded score map and one of the ground truth boxes is at least the \acrshort{iou} threshold. \acrshort{maxboxacc} is then the box accuracy at the optimal threshold. Choe \textit{et al.} propose an improved version called \acrshort{maxboxacc}V2. It is the average of \acrshort{maxboxacc} across the \acrshort{iou} thresholds 0.3, 0.5 and 0.7 to address different granularity of objects in an image. \acrshort{maxboxacc}V2 also removes the assumption that the object of interest is usually large by considering the best match between the set of estimated boxes in the score map and the set of ground truth boxes. \acrshort{maxboxacc} and \acrshort{maxboxacc}V2 are score map threshold independent. MinMaxCAM \cite{wang2021minmaxcam} uses the metrics proposed by Choe \textit{et al.}.

\subsection{Evaluation of multiple instances}
Many \acrshort{cam}-based papers use different evaluation metrics to measure localization performance. Choe \textit{et al.} \cite{choe2020evaluating} propose a unified definition of the \acrshort{wsol} task and a new localization evaluation protocol. They also benchmark \acrshort{cam}-based methods using their proposed evaluation protocol. However, the metrics used in that protocol don't measure the localization of multiple object instances of the same class.

\section{Our contributions}
In our work we are evaluating \acrshort{wsol} using the set of \acrshort{cam}-based methods described in this chapter. We evaluate the performance of multiple-instance localization and call this task \acrfull{mwsol}. As prior evaluation metrics are not sufficient for this task, we propose an evaluation protocol for localization of multiple object instances of the same class. 

We enhance existing evaluation metrics \cite{choe2020evaluating} to measure multiple-instance localization performance. When ground truth segmentation masks are available we will reuse \textbf{\acrshort{pxap}} as defined by Choe \textit{et al.}. As semantic segmentation masks are instance-unaware, we can use this metric as is. When ground truth bounding boxes are available for evaluation, we enhance \textbf{\acrshort{maxboxacc}V2} to measure localization of multiple object instances per image and call this metric \textbf{\acrshort{maxboxacc}V3}. 

We benchmark existing \acrshort{cam}-based methods for multiple-instance localization. Finally, we investigate improvements for the \acrshort{mwsol} task using an iterative bounding box extraction method. Location information of objects localized in previous iterations, is used to mask those objects in images. Those masked images are then used to find the location of objects missed during previous iterations.