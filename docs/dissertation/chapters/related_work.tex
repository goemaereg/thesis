\chapter{Related work} \label{ch:related_work}
\section{Object recognition}
\acrfull{cnn}s are used with great performance in a number of computer vision tasks. Within the landscape of object recognition tasks, given an input image, classification models return image-level class predictions \cite{lecun1998gradient}, semantic segmentation models \cite{shelhamer2017fully} produce pixel-wise class predictions, and object detection models \cite{girshick2014rich, girshick2015fast, ren2015faster} output a set of bounding-boxes with class predictions, one bounding box and class per recognized object. Object localization \cite{russakovsky2015imagenet} assumes that the image contains an object of a single class and produces a binary mask or a bounding box around the object of the class of interest.

\section{Weakly supervised object localization}
\textbf{\acrshort{wsol}}. In supervised learning, a model learns a task by minimizing the error between its predictions and ground-truth labels. As human labeling of bounding boxes and segmentation masks cost significantly more than image-level categories \cite{papandreou2015weakly}. Because \acrshort{wsol} only requires image-level class labels as ground-truth, \acrshort{wsol} research \cite{zhou2016cvpr, zhang2018adversarial, zhang2018self, choe2019attention, singh2017hide} has gained significant momentum.

\section{CAM-based WSOL methods}
\subsection{Class Activation Mapping}
\textbf{\acrshort{cam} related work}. An important \acrshort{wsol} method and baseline for a family of \acrshort{cam} methods was introduced by Zhou \textit{et al.} \cite{zhou2016cvpr}. \acrfull{cam} uses feature map activations in a \acrlong{cnn} to localize the discriminative image regions despite being trained for a classification task. A disadvantage is that \acrshort{cam} requires modifications in network architectures to accomodate a \acrlong{gap} layer. This has as consequence that classificiation accuracy drops compared with the original architecture. Another disadvantage is that \acrshort{cam} can only partially localize objects due to its focus on discriminative parts of an image. 

\subsection{Gradient-based CAM}
New \acrshort{wsol} research focused on overcoming the limitations of \acrshort{cam}. \acrfull{gradcam}, proposed by Selvaraju \textit{et al.} \cite{selvaraju2017grad}, uses gradients of a target with respect to the final convolutional layer to produce a localization map highlighting the important regions in an image concerning a certain object target. \acrshort{gradcam} is a generalization of \acrshort{cam}: It is applicable to a wide variety of \acrshort{cnn} architecture as it doesn't require a specific \acrshort{gap} layer. Off-the-shelf models can be evaluated for the \acrshort{wsol} task as no retraining is required. \acrshort{gradcam} is also used for making \acrshort{cnn}-based models more transparent by producing visual explanations. Further, \acrshort{gradcam} combines localizations with existing high-resolution visualizations (Springenberg, J.T. \textit{et al.} \cite{springenberg2014striving}) to obtain high-resolution class-discriminative Guided Grad-CAM visualizations.

Chattopadhay \textit{et al.} \cite{chattopadhay2018grad}) introduced GradCAM++, a generalization of \acrshort{gradcam}. It addresses GradCAM's shortcomings that multiple occurrences of objects of the same class in an image are poorly localized. As in \acrshort{gradcam}, the CAM of a certain class is a weighted combination of activation maps in the highest convolutional layer. In Grad-CAM, weights are an average of the gradients of an activation map. In Grad-CAM++, weights are a weighted average of the gradients of each pixel in the activation map. This way, parts of the image important for predicting a specific class will be highlighted more in the activation map. Grad-CAM++ provides better visual explanations for a given \acrshort{cnn} architecture when compared to Grad-CAM.

\subsection{CAM regularization}
One of the most common problems of \acrshort{cam} methods is caused either by localization maps which focus, exclusively, on the most discriminative region of the objects of interest, or by activations occurring in background regions. MinMaxCAM, proposed by Wang \textit{et al.} \cite{wang2021minmaxcam}, addresses these two problems by proposing two representation regularization mechanisms: Full Region Regularization which tries to maximize the coverage of the localization map inside the object region, and Common Region Regularization which minimizes the activations occurring in background region. MinMaxCAM has the same architectural constraints as \acrshort{cam} in that it requires a modified network architecture with a \acrshort{gap} layer. An additional constraint is that it requires re-training due to its region regularization.

\subsection{Score-based CAM}
Wang \textit{et al.} \cite{wang2020score} propose a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike some previous \acrshort{cam} methods, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class. The final score map is obtained by a weighted combination of activation maps. Score-CAM achieves better visual performance and fairness for interpreting the decision making process and outperforms previous methods on localization tasks. It also shows promise to recognize multiple instances of the same class within an image. A disadvantage of the method is its high computational cost because it requires as many forward passes as there are activation maps to compute the weight of each activation map.

\textbf{Explainability}. Some \acrshort{wsol} methods \cite{chattopadhay2018grad, wang2020score} improve explainability of deep \acrshort{cnn}. There are similarities between the \acrshort{wsol} task and explainability: I.e. analysing which pixels lead to image classification results. We focus on the quantitative evaluation of the \acrshort{wsol} for the \acrshort{cam} methods described in chapter \ref{ch:methodology}.

\section{Non-CAM WSOL methods}
There has been a number of works other than \acrshort{cam} methods exploring \acrfull{wsol} using \acrshort{cnn}s. Bergamo \textit{et al.} \cite{bazzani2016self} proposes self-taught object localization technique that masks out image regions to identify the region that incurs the maximal drop in recognition score, and hence has the highest likelihood to contain the object of interest. Cinbis \textit{et al.} \cite{cinbis2016weakly} and Pinheiro \textit{et al.} \cite{pinheiro2015image} combine multiple-instance learning with \acrshort{cnn}s to localize objects. While these approaches yield some promising results, they are not trained end-to-end.

\section{WSOL evaluation}
\subsection{Localization error}
Localization performance in \textbf{\acrshort{cam}} \cite{zhou2016cvpr} and \textbf{\acrshort{gradcam}} \cite{selvaraju2017grad} is computed by comparing the \acrfull{iou} between the ground-truth bounding box and the bounding box that covers the largest connected component in the activation map with values of at least 20\% of the maximum value. An object is considered localized under the 0.5 \acrshort{iou} criterion, i.e. ground-truth and derived bounding box overlap at least 50\%. This computation is done for each of the top-5 predicted classes for the top-5 localization evaluation metric \cite{russakovsky2015imagenet}. The top-1 localization error is then the localization error for the top-1 predicted class.

\subsection{Faithfulness based metrics}
Chattopadhay \textit{et al.} \cite{chattopadhay2018grad}) evaluate the faithfulness of generated explanations. An explanation map is generated by the Hadamard product \cite{million2007hadamard} of the upsampled class-discriminative saliency map with the original image. The explanation map is used as input to the model to evalutate three different metrics.

\textbf{Average drop\%} This metric compares the average percentage drop in the model's prediction score for a particular class in an image when using the explanation map (i.e. after occlusion of the image with the saliency map). The better the explanation map keeps the most important image regions for a class, the less the fall of the prediction score is to be expected.

\textbf{\% increase in confidence:} This metric measures the number of times in a dataset the model's confidence increased when occluding unimportant regions. This happens when all patterns a deep \acrshort{cnn} looks for are highlighted by the explanation map.

\textbf{Win\%:} This metric measures the number of times, in a given dataset, the drop in the model's confidence for an explanation map generated by \acrshort{gradcam}++ is less than for the explanation map generated by \acrshort{gradcam}. A lower fall indicates that the explanation of \acrshort{gradcam}++ is more model-appropriate than the explanation of \acrshort{gradcam}.

\subsection{Localization evaluation from energy-based perspective}
\textbf{ScoreCAM} evaluates faithfulness of explanations using the same metrics as \acrshort{gradcam}++. For localization evaluation ScoreCAM uses a metric that treats localization from an energy based perspective: The metric computes the proportion of the sum of pixels in the saliency map that are inside a ground truth bounding box versus the sum of all pixels of the saliency map. For convenience, ScoreCAM only considers these images with only one bounding box for a target class.

\subsection{Threshold-independent evaluation metrics}
\textbf{Choe \textit{et al.} \cite{choe2020evaluating}} argue that using a fixed predefined threshold (Zhou \textit{et al.} \cite{zhou2016cvpr}) to produce a \acrshort{cam} localization region can be disadvantageous as the ideal threshold may depend on the data and architecture used. They discuss that prior evaluation metrics have failed to clearly measure localization performance. The localization accuracy metric \cite{russakovsky2015imagenet} and localization error \cite{zhou2016cvpr, selvaraju2017grad} metric combine classification and localization performances by counting the number of images where both tasks are performed correctly. They advocate to measure localization performance alone as the goal of \acrshort{wsol} is to localize objects and not to classify correctly. Hence, only score maps corresponding to ground-truth classes are considered. Corresponding metrics are referred to as \textit{GT-known} metrics \cite{choe2019attention, singh2017hide, zhang2018adversarial, zhang2018self}. Choe \textit{et al.} proposes new evaluation metrics that are threshold-independent.

\subsubsection{Pixel-wise average precision}
When ground-truth masks are available for evaluation, the pixel-wise precision and recall \cite{achanta2009frequency} are measured. This allows users to choose the preferred operating threshold that provides the best precision-recall trade-off. For threshold independence, the \acrfull{pxap} is defined as the area under the pixel-wise precision-recall curve.

\subsubsection{Maximum box accuracy}
Given availability of a ground-truth bounding box, the box accuracy at a specific score map threshold and for a specific \acrshort{iou} threshold is then the number of images in a dataset where the overlap  between the bounding box of the largest connected component of the thresholded scoremap and one of the ground-truth boxes is at least the \acrshort{iou} threshold. \acrshort{maxboxacc} is then the box accuracy at the optimal threshold. Choe \textit{et al.} propose an improved version called \acrshort{maxboxacc}V2. It is the average of \acrshort{maxboxacc} across the \acrshort{iou} thresholds 0.3, 0.5 and 0.7 to address different granularity of objects in an image. \acrshort{maxboxacc}V2 also removes the assumption that the object of interest is usually large by considering the best match between the set of estimated boxes in the score map and the set of ground-truth boxes. \acrshort{maxboxacc} and \acrshort{maxboxacc}V2 are score map threshold independent. \textbf{MinMaxCAM \cite{wang2021minmaxcam}} follows uses the metrics proposed by Choe \textit{et al.}.

\subsection{Lacking evaluation of multiple-instances}
Many \acrshort{cam} papers use different evaluation metrics to measure localization performance. Choe \textit{et al.} \cite{choe2020evaluating} propose a unified definition of the \acrshort{wsol} task and a new localization evaluation protocol. They also benchmark \acrshort{cam} related \acrshort{wsol} tasks. However, the metrics used don't measure the localization of multiple object instances of the same class.

\section{Our contributions}
In our work we are evaluating \acrshort{wsol} tasks using a the set of \acrshort{cam} methods described in this chapter. We evaluate the performance of multiple-instance localization and call this task \acrfull{mwsol}. As the prior evaluation metrics are not sufficient for this task, we propose an evaluation protocol for localization of multiple instances of the same class. 

We enhance existing evaluation metrics \cite{choe2020evaluating} to measure multiple-instance localization performance. When ground-truth masks are available we will reuse \textbf{\acrshort{pxap}} as defined by Choe \textit{et al.}. As semantic segmentation masks are instance-unaware, we can use this metric as is. When ground-truth bounding boxes are available for evaluation, we enhance \textbf{\acrshort{maxboxacc}V2} to measure localization of multiple instances per image and call this metric \textbf{\acrshort{maxboxacc}V3}. 

We benchmark existing \acrshort{cam} methods for multiple-instance localization. Finally, we investigate improvements for the \acrshort{mwsol} task using an iterative bounding box extraction method. Bounding boxes of objects localized in previous iterations, are used to mask their location in images which are then used to find the location of objects missed during previous iterations.